{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCKCzNru0dA2",
        "outputId": "b5869c90-e6b0-48ce-98f2-4e067dce9211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import math\n",
        "from scipy.special import softmax\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')  # Scarica i dati necessari per la tokenizzazione\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us code the self attention with toy examples and visualize attention values"
      ],
      "metadata": {
        "id": "9x-BsCxS1DA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d=100 # Embedding dimension\n",
        "Q=np.random.rand(d,d)\n",
        "K=np.random.rand(d,d)\n",
        "V=np.random.rand(d,d)\n",
        "# In this scenario k=d because we are using just one head and we recall that k=d/H, d over the number of heads\n",
        "\n",
        "def attention_values(Q,K,V,X):\n",
        "    query = np.dot(Q,X)\n",
        "    key = np.dot(K,X)\n",
        "    value = np.dot(V,X)\n",
        "    k=X.shape[1] # the embedding dimension of the input X\n",
        "    att_w = np.dot(query,key.T)/math.sqrt(k) # computation of att_weights\n",
        "    att_w = softmax(att_w, axis=1) #we compute the softmax by rows.\n",
        "    att_values = np.dot(att_w,value)\n",
        "    return att_values\n",
        "\n",
        "def visualize(att_values,sentence):\n",
        "    sns.heatmap(att_values, cmap=\"crest\", xticklabels=word_tokenize(sentence.lover()), yticklabels=word_tokenize(sentence.lover()))\n",
        "\n"
      ],
      "metadata": {
        "id": "LNNuEZ1_1QLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "\n",
        "sentence = #here we choose a sentence to visualize the attention values.\n",
        "n=len(sentence) # so X would be a matrix of dimension (n,d)\n",
        "\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus] # we tokenize each sentence in the corpus.\n",
        "\n",
        "# We create a dictionary and one-hot encode the sentence\n",
        "token_index = {}\n",
        "counter = 0 # Create a counter for counting the number of key-value pairs in the token_length\n",
        "\n",
        "# Select the elements of the samples which are the two sentences\n",
        "for sentence in tokenized_corpus:\n",
        "  for word in sentence:\n",
        "    if word not in token_index:\n",
        "      # If the considered word is not present in the dictionary token_index, add it to the token_index\n",
        "      # The index of the word in the dictionary begins from 1\n",
        "      token_index.update({word : counter + 1})\n",
        "      # updating the value of counter\n",
        "      counter = counter + 1\n",
        "\n",
        "# Now token_index can be used to represent words as one-hot encode\n",
        "\n",
        "def one_hot_sentence(token_index, sentence):\n",
        "    one_hot=np.zeros(len(sentence),max(token_index.values()))\n",
        "    for i,word in enumerate(sentence):\n",
        "      one_hot[i,token_index(word)]=1\n",
        "    return one_hot\n",
        "\n",
        "# The one-hot encoding must be of lenght d so we have to go from (vocab_size)---> d using a linea map W_E\n",
        "W_e=np.random.rand(token_index.values(),d)\n"
      ],
      "metadata": {
        "id": "wkXwYMU358xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hoWNqkNoHepl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}